Your all-in-one GenAI-powered research companion that can:

Parse and summarize uploaded documents

Answer custom questions about them

Generate logic-based quiz questions

Evaluate your answers with feedback





**This is a strucutre for my code.**

 Layer -- 	                         Component --	                                       Purpose
 
ğŸ“ app.py --	                    Streamlit UI	  --	                               Main frontend, controls sidebar, user input, layout

ğŸ“ backend/parser.py --	        extract_text()   --	                             	Reads and extracts raw text from uploaded PDFs or .txt

ğŸ“ backend/summarizer.py --		    generate_summary()	 --	                            Sends raw doc to LLM to return TL;DR-style summary

ğŸ“ backend/qa_engine.py --		        answer_question()  --	                           	Handles Q&A from doc using FAISS + LLM prompt

ğŸ“ backend/challenge_engine.py	generate_questions() / evaluate_answer()	 --	   Quiz generator and feedback engine

ğŸ“ utils/retrieval.py --		        split_text(), build_faiss_index(), search_top_k()  --	   	Vector-based chunk search logic

ğŸ“ utils/validator.py --		        is_answer_plausible()	  --	                        Checks how grounded a response is in context (using difflib)

ğŸ“¦ Model API	 --	                Mistral-7B via OpenRouter	 --	                     Handles summarization, question answering, and evaluation





**concept of working of m model**

ğŸ” **Flow Example: "Summarize + Ask"**
User uploads a file â†’ extract_text() reads it

Text gets summarized â†’ generate_summary()

User asks a question â†’ stored in st.session_state.chat_history

qa_engine.py:

Breaks document into chunks

Finds top similar chunks with FAISS

Builds LLM prompt with context + history

Sends prompt to Mistral

Returns answer + justification (top chunk)

Session state keeps the full back-and-forth chat memory





ğŸ§© **Flow Example: "Challenge Me"**
App sends context (first 5 chunks) to LLM â†’ generates MCQ-style questions

User answers â†’ evaluation prompt is sent

LLM judges the answer and gives feedback

Validator soft-checks whether that feedback matches context



Setup Instructions

Create and activate a virtual environment

**(Python 3.11.9 recommended)**

python -m venv venv

source venv/bin/activate  # or venv\Scripts\activate on Windows





Install required dependencies
pip install -r requirements.txt


Run the app
streamlit run app.py





                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚     ğŸ“„ Uploaded Document    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                        Text Extraction (parser.py)
                                   â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ generate_summary()   â”‚ â† Summarizer (summarizer.py)
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                    â”‚
        â–¼                                                    â–¼
 Q&A Mode (qa_engine.py)                            Challenge Mode (challenge_engine.py)
        â”‚                                                    â”‚
  Split into chunks                                Extract 5 chunks
  Build prompt w/ chat history                     Prompt LLM for questions
  Retrieve top-k similar context                   Let user answer â†’ Evaluate
  Send to Mistral (OpenRouter)                     Soft-check answer grounding
        â”‚                                                    â”‚
        â–¼                                                    â–¼
 Return answer + source                          Return feedback
        â”‚
 Silent plausibility checker (validator.py)





in theo , explained above 

smart-research-assistant/
â”‚
â”œâ”€â”€ app.py                         # Main Streamlit UI logic
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ parser.py                  # Extracts text from PDFs and text files
â”‚   â”œâ”€â”€ summarizer.py              # Summarizes text using OpenRouter
â”‚   â”œâ”€â”€ qa_engine.py               # Handles Q&A using document + chat history
â”‚   â””â”€â”€ challenge_engine.py        # Generates and evaluates logic questions
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ retrieval.py               # Chunk splitter and context retriever
â”‚   â””â”€â”€ validator.py               # Checks LLM outputs for hallucination
â”‚
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml               # (optional) API key storage
â”‚
â””â”€â”€ requirements.txt               # Python dependencies


Built by Nilanchal Upadhyay




